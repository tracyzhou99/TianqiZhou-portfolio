{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ef44f5-d97b-4b4e-bfe9-f5a6b6304a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.11.17         |  py310h06a4308_0         158 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       341.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> pkgs/main::ca-certificates-2023.08.22-h06a4308_0 \n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/linux-64::certifi-2023.11.17-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2023.11.17   | 158 KB    |                                       |   0% \n",
      "openjdk-11.0.13      | 341.0 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "certifi-2023.11.17   | 158 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | 5                                     |   1% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##1                                   |   6% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###5                                  |  10% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #####3                                |  14% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #######3                              |  20% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #########3                            |  25% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###########                           |  30% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ############8                         |  35% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############5                       |  39% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ################4                     |  44% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################1                   |  49% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################8                  |  54% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #####################5                |  58% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #######################2              |  63% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ########################9             |  67% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########################8           |  72% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ############################7         |  78% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############################5       |  83% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ################################3     |  87% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################################1   |  92% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################################8  |  97% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spark-nlp==5.1.3\n",
      "  Obtaining dependency information for spark-nlp==5.1.3 from https://files.pythonhosted.org/packages/cd/7d/bc0eca4c9ec4c9c1d9b28c42c2f07942af70980a7d912d0aceebf8db32dd/spark_nlp-5.1.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached spark_nlp-5.1.3-py2.py3-none-any.whl.metadata (53 kB)\n",
      "Using cached spark_nlp-5.1.3-py2.py3-none-any.whl (537 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# install spark-nlp\n",
    "%pip install spark-nlp==5.1.3\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c41c91-3440-47e8-90f6-119364576383",
   "metadata": {},
   "source": [
    "## Process the dataset using job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be7f1b-5612-4213-89b8-1cbcb372f731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "2023-11-29 01:39:46  708534094 spark-nlp-assembly-5.1.3.jar\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "!wget -qO- https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-5.1.3.jar | aws s3 cp - s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\n",
    "!aws s3 ls s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea86ad6-a2b1-48d8-a536-78188cc2eae0",
   "metadata": {},
   "source": [
    "## Process random forest regression with max_depth = 5, max_bins=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208bb134-8acc-4bdb-9d38-912add65e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/rf-5-10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/rf-5-10.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('subreddit', StringType(), True),\n",
    "            StructField('id', StringType(), True),\n",
    "            StructField('body', StringType(), True),\n",
    "            StructField('score', DoubleType(), True),\n",
    "            StructField('gilded', DoubleType(), True),\n",
    "            StructField('score', IntegerType(), True),\n",
    "            StructField('sentiment_index', StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True, schema=schema)\n",
    "    df = df.repartition(64)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # string indexer for subreddit\n",
    "    string_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"subreddit_index\")\n",
    "    # fit and transform the DataFrame\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "    # string indexer for sentiment\n",
    "    string_indexer = StringIndexer(inputCol=\"sentiment_index\", outputCol=\"sentiments_index\")\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "    # build pipeline for bert sentence embeddings\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"body\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\")\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"sentence_bert_embeddings\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher()\\\n",
    "      .setInputCols(\"sentence_bert_embeddings\")\\\n",
    "      .setOutputCols(\"finished_embeddings\")\\\n",
    "      .setOutputAsVector(True)\n",
    "\n",
    "    pipeline = Pipeline().setStages([\n",
    "        documentAssembler,\n",
    "        embeddings,\n",
    "        embeddingsFinisher,\n",
    "    ])\n",
    "    result = pipeline.fit(df).transform(df)\n",
    "    # explode the result into a vector\n",
    "    result = result.withColumn(\"finished_embeddings_vector\", explode(\"finished_embeddings\"))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"finished_embeddings_vector\", \"subreddit_index\",\"sentiments_index\",'gilded',\"is_submission\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    result = assembler.transform(result)\n",
    "    result.persist()\n",
    "    \n",
    "    # split the data into train and test set\n",
    "    train_df, test_df = result.randomSplit([0.7, 0.3], seed=232)\n",
    "    \n",
    "    # define the model\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"score\",\n",
    "        maxDepth=5,\n",
    "        maxBins=10\n",
    "    )\n",
    "\n",
    "    # fit the pipeline\n",
    "    model = rf.fit(train_df)\n",
    "\n",
    "    # make predictions on data\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # get the predictions\n",
    "    output = predictions.select(\"score\", \"prediction\")\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}/rf-5-10\"\n",
    "    logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "    output.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    logger.info(\"all done\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6f6869-2a3e-4319-8692-42e8f651c732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "account_id=260236516028, s3_dataset_path=s3://sagemaker-us-east-1-260236516028/6000_project/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-rf-2023-11-29-02-14-57-295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................!CPU times: user 812 ms, sys: 75.6 ms, total: 888 ms\n",
      "Wall time: 10min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-rf\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path = f\"*\"\n",
    "s3_dataset_path_1 = f\"*\"\n",
    "print(f\"account_id={account_id}, s3_dataset_path={s3_dataset_path_1}\")\n",
    "output_prefix_data = f\"result/rf\"\n",
    "output_prefix_logs = f\"result/logs\"\n",
    "\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"../scripts/rf-5-10.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_dataset_path,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        output_prefix_data,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91017b7-2054-421e-af8a-e69ab627a0bd",
   "metadata": {},
   "source": [
    "## Process random forest regression with max_depth = 10, max_bins=30 and SAVE the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58879272-6360-4c49-b3d4-bfbcba5806c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/rf-10-30-save.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/rf-10-30-save.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "    df = df.repartition(64)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # string indexer for subreddit\n",
    "    string_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"subreddit_index\")\n",
    "    # fit and transform the DataFrame\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "    # string indexer for sentiment\n",
    "    string_indexer = StringIndexer(inputCol=\"sentiment_index\", outputCol=\"sentiments_index\")\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "    # build pipeline for bert sentence embeddings\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"body\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\")\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"sentence_bert_embeddings\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher()\\\n",
    "      .setInputCols(\"sentence_bert_embeddings\")\\\n",
    "      .setOutputCols(\"finished_embeddings\")\\\n",
    "      .setOutputAsVector(True)\n",
    "\n",
    "    pipeline = Pipeline().setStages([\n",
    "        documentAssembler,\n",
    "        embeddings,\n",
    "        embeddingsFinisher,\n",
    "    ])\n",
    "    result = pipeline.fit(df).transform(df)\n",
    "    # explode the result into a vector\n",
    "    result = result.withColumn(\"finished_embeddings_vector\", explode(\"finished_embeddings\"))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"finished_embeddings_vector\", \"subreddit_index\",\"sentiments_index\",'gilded',\"is_submission\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    result = assembler.transform(result)\n",
    "    result.persist()\n",
    "    \n",
    "    # split the data into train and test set\n",
    "    train_df, test_df = result.randomSplit([0.7, 0.3], seed=232)\n",
    "    \n",
    "    # define the model\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"score\",\n",
    "        maxDepth=10,\n",
    "        maxBins=30\n",
    "    )\n",
    "\n",
    "    # fit the pipeline\n",
    "    model = rf.fit(train_df)\n",
    "\n",
    "    # make predictions on data\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # get the predictions\n",
    "    output = predictions.select(\"score\", \"prediction\")\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}/rf-10-30\"\n",
    "    model_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}/rfmodel\"\n",
    "    model.save(model_path)\n",
    "    logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "    output.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    logger.info(\"all done\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0fa9f3e-fc24-4405-beb2-2c6b87db2cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "account_id=260236516028, s3_dataset_path=s3://sagemaker-us-east-1-640225923506/6000_project/ml_cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-rf-2023-12-08-07-54-31-807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................!CPU times: user 3.32 s, sys: 198 ms, total: 3.52 s\n",
      "Wall time: 11min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-rf\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "output_bucket = \"*\"\n",
    "s3_dataset_path = f\"*\"\n",
    "print(f\"account_id={account_id}, s3_dataset_path={s3_dataset_path}\")\n",
    "output_prefix_data = f\"6000_project\"\n",
    "output_prefix_logs = f\"result/logs\"\n",
    "\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"../scripts/rf-10-30-save.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_dataset_path,\n",
    "        \"--s3_output_bucket\",\n",
    "        output_bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        output_prefix_data,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b742431e-9bd3-4214-81e0-af7934c650b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/rf-10-50.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/rf-10-30.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('subreddit', StringType(), True),\n",
    "            StructField('id', StringType(), True),\n",
    "            StructField('body', StringType(), True),\n",
    "            StructField('score', DoubleType(), True),\n",
    "            StructField('gilded', DoubleType(), True),\n",
    "            StructField('score', IntegerType(), True),\n",
    "            StructField('sentiment_index', StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True, schema=schema)\n",
    "    df = df.repartition(64)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # string indexer for subreddit\n",
    "    string_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"subreddit_index\")\n",
    "    # fit and transform the DataFrame\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "    # string indexer for sentiment\n",
    "    string_indexer = StringIndexer(inputCol=\"sentiment_index\", outputCol=\"sentiments_index\")\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "    # build pipeline for bert sentence embeddings\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"body\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\")\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"sentence_bert_embeddings\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher()\\\n",
    "      .setInputCols(\"sentence_bert_embeddings\")\\\n",
    "      .setOutputCols(\"finished_embeddings\")\\\n",
    "      .setOutputAsVector(True)\n",
    "\n",
    "    pipeline = Pipeline().setStages([\n",
    "        documentAssembler,\n",
    "        embeddings,\n",
    "        embeddingsFinisher,\n",
    "    ])\n",
    "    result = pipeline.fit(df).transform(df)\n",
    "    # explode the result into a vector\n",
    "    result = result.withColumn(\"finished_embeddings_vector\", explode(\"finished_embeddings\"))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"finished_embeddings_vector\", \"subreddit_index\",\"sentiments_index\",'gilded',\"is_submission\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    result = assembler.transform(result)\n",
    "    result.persist()\n",
    "    \n",
    "    # split the data into train and test set\n",
    "    train_df, test_df = result.randomSplit([0.7, 0.3], seed=232)\n",
    "    \n",
    "    # define the model\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"score\",\n",
    "        maxDepth=10,\n",
    "        maxBins=30，\n",
    "    )\n",
    "\n",
    "    # fit the pipeline\n",
    "    model = rf.fit(train_df)\n",
    "\n",
    "    # make predictions on data\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # get the predictions\n",
    "    output = predictions.select(\"score\", \"prediction\")\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}/rf-10-30\"\n",
    "    logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "    output.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    logger.info(\"all done\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ad478c6-002a-4bf7-b9a9-d80a66186955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "account_id=260236516028, s3_dataset_path=s3://sagemaker-us-east-1-260236516028/6000_project/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-rf-2023-11-29-03-06-36-641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................................................................!CPU times: user 884 ms, sys: 106 ms, total: 990 ms\n",
      "Wall time: 10min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-rf\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path = f\"*\"\n",
    "s3_dataset_path_1 = f\"*\"\n",
    "print(f\"account_id={account_id}, s3_dataset_path={s3_dataset_path_1}\")\n",
    "output_prefix_data = f\"result/rf\"\n",
    "output_prefix_logs = f\"result/logs\"\n",
    "\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"../scripts/rf-10-30.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_dataset_path,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        output_prefix_data,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daeae2e-6f49-4d17-a6f9-e94c834715fa",
   "metadata": {},
   "source": [
    "## Process linear regression maxIter=10, regParam=0.3, elasticNetParam=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b701ccb1-8552-4468-a78a-92907c769bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/lr-10-0.3-0.8.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/lr-10-0.3-0.8.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('subreddit', StringType(), True),\n",
    "            StructField('id', StringType(), True),\n",
    "            StructField('body', StringType(), True),\n",
    "            StructField('score', DoubleType(), True),\n",
    "            StructField('gilded', DoubleType(), True),\n",
    "            StructField('score', IntegerType(), True),\n",
    "            StructField('sentiment_index', StringType(), True),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True, schema=schema)\n",
    "    df = df.repartition(64)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # string indexer for subreddit\n",
    "    string_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"subreddit_index\")\n",
    "    # fit and transform the DataFrame\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "    # string indexer for sentiment\n",
    "    string_indexer = StringIndexer(inputCol=\"sentiment_index\", outputCol=\"sentiments_index\")\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "    # build pipeline for bert sentence embeddings\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"body\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\")\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"sentence_bert_embeddings\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher()\\\n",
    "      .setInputCols(\"sentence_bert_embeddings\")\\\n",
    "      .setOutputCols(\"finished_embeddings\")\\\n",
    "      .setOutputAsVector(True)\n",
    "\n",
    "    pipeline = Pipeline().setStages([\n",
    "        documentAssembler,\n",
    "        embeddings,\n",
    "        embeddingsFinisher,\n",
    "    ])\n",
    "    result = pipeline.fit(df).transform(df)\n",
    "    # explode the result into a vector\n",
    "    result = result.withColumn(\"finished_embeddings_vector\", explode(\"finished_embeddings\"))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"finished_embeddings_vector\", \"subreddit_index\",\"sentiments_index\",'gilded',\"is_submission\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    result = assembler.transform(result)\n",
    "    result.persist()\n",
    "    \n",
    "    # split the data into train and test set\n",
    "    train_df, test_df = result.randomSplit([0.7, 0.3], seed=232)\n",
    "    \n",
    "    # define the model\n",
    "    lr = LinearRegression(maxIter=5, regParam=0.3, elasticNetParam=0.8,\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"score\",\n",
    "    )\n",
    "\n",
    "    # fit the pipeline\n",
    "    model = lr.fit(train_df)\n",
    "\n",
    "    # make predictions on data\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # get the predictions\n",
    "    output = predictions.select(\"score\", \"prediction\")\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}/lr-10-0.3-0.8\"\n",
    "    logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "    output.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    logger.info(\"all done\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5db630e6-78aa-41df-86b0-922a3e82398a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "account_id=260236516028, s3_dataset_path=s3://sagemaker-us-east-1-260236516028/6000_project/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-tree-2023-11-29-03-19-07-162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................................!CPU times: user 1.01 s, sys: 87.9 ms, total: 1.09 s\n",
      "Wall time: 12min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-tree\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path = f\"*\"\n",
    "s3_dataset_path_1 = f\"*\"\n",
    "print(f\"account_id={account_id}, s3_dataset_path={s3_dataset_path_1}\")\n",
    "output_prefix_data = f\"result/lr\"\n",
    "output_prefix_logs = f\"result/logs\"\n",
    "\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"../scripts/lr-10-0.3-0.8.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_dataset_path,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        output_prefix_data,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f3c6f-dec3-4c57-8688-c5fae01760d5",
   "metadata": {},
   "source": [
    "## Process linear regression maxIter=10, regParam=0.3, elasticNetParam=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9786f53-97e1-4aff-9d22-1490d79032fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/lr-10-0.3-0.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/lr-10-0.3-0.2.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField('subreddit', StringType(), True),\n",
    "            StructField('id', StringType(), True),\n",
    "            StructField('body', StringType(), True),\n",
    "            StructField('score', DoubleType(), True),\n",
    "            StructField('gilded', DoubleType(), True),\n",
    "            StructField('score', IntegerType(), True),\n",
    "            StructField('sentiment_index', StringType(), True),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True, schema=schema)\n",
    "    df = df.repartition(64)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # string indexer for subreddit\n",
    "    string_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"subreddit_index\")\n",
    "    # fit and transform the DataFrame\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "    # string indexer for sentiment\n",
    "    string_indexer = StringIndexer(inputCol=\"sentiment_index\", outputCol=\"sentiments_index\")\n",
    "    df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "    # build pipeline for bert sentence embeddings\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"body\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    embeddings = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\")\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"sentence_bert_embeddings\")\n",
    "\n",
    "    embeddingsFinisher = EmbeddingsFinisher()\\\n",
    "      .setInputCols(\"sentence_bert_embeddings\")\\\n",
    "      .setOutputCols(\"finished_embeddings\")\\\n",
    "      .setOutputAsVector(True)\n",
    "\n",
    "    pipeline = Pipeline().setStages([\n",
    "        documentAssembler,\n",
    "        embeddings,\n",
    "        embeddingsFinisher,\n",
    "    ])\n",
    "    result = pipeline.fit(df).transform(df)\n",
    "    # explode the result into a vector\n",
    "    result = result.withColumn(\"finished_embeddings_vector\", explode(\"finished_embeddings\"))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"finished_embeddings_vector\", \"subreddit_index\",\"sentiments_index\",'gilded',\"is_submission\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    result = assembler.transform(result)\n",
    "    result.persist()\n",
    "    \n",
    "    # split the data into train and test set\n",
    "    train_df, test_df = result.randomSplit([0.7, 0.3], seed=232)\n",
    "    \n",
    "    # define the model\n",
    "    lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.2,\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"score\",\n",
    "    )\n",
    "\n",
    "    # fit the pipeline\n",
    "    model = lr.fit(train_df)\n",
    "\n",
    "    # make predictions on data\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # get the predictions\n",
    "    output = predictions.select(\"score\", \"prediction\")\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}/lr-10-0.3-0.2\"\n",
    "    logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "    output.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    logger.info(\"all done\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d6b5fd8-3f6f-4fee-a260-36186a28bd9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "account_id=260236516028, s3_dataset_path=s3://sagemaker-us-east-1-260236516028/6000_project/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-lr-2023-11-29-03-47-54-747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................!CPU times: user 852 ms, sys: 88.7 ms, total: 941 ms\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-lr\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path = f\"*\"\n",
    "s3_dataset_path_1 = f\"*\"\n",
    "print(f\"account_id={account_id}, s3_dataset_path={s3_dataset_path_1}\")\n",
    "output_prefix_data = f\"result/lr\"\n",
    "output_prefix_logs = f\"result/logs\"\n",
    "\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"../scripts/lr-10-0.3-0.2.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_dataset_path,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        output_prefix_data,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fdd543-122a-4549-b54b-684901e5f491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2a57cd82-c304-40fc-8c83-7c5d0fc51e56;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 311ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2a57cd82-c304-40fc-8c83-7c5d0fc51e56\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/13ms)\n",
      "23/12/08 18:19:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d5e61e-fc38-4ce8-9686-bc212cc35e9e",
   "metadata": {},
   "source": [
    "## RMSE for random forest maxdepth=5, maxbins=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19192ea-f637-4665-80f2-690e4ee201fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading results from s3a://sagemaker-us-east-1-260236516028/result/rf/rf-5-10/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 05:07:41 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 88,233x2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.95315\n",
      "CPU times: user 1.47 s, sys: 283 ms, total: 1.75 s\n",
      "Wall time: 16.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"*\"\n",
    "print(f\"reading results from {s3_path}\")\n",
    "rf_1 = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {rf_1.count():,}x{len(rf_1.columns)}\")\n",
    "# calculate the rmse\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_1)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe2613-d7d3-4117-bbe4-c6439482012a",
   "metadata": {},
   "source": [
    "## RMSE for random forest maxdepth=10, maxbins=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659a1419-6a4b-4c8b-98ea-3e13952936ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading results from s3a://sagemaker-us-east-1-260236516028/result/rf/rf-10-30/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 88,233x2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.90691\n",
      "CPU times: user 11.3 ms, sys: 7.87 ms, total: 19.2 ms\n",
      "Wall time: 7.38 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"*\"\n",
    "print(f\"reading results from {s3_path}\")\n",
    "rf_2 = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {rf_2.count():,}x{len(rf_2.columns)}\")\n",
    "# calculate the rmse\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_2)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e52a6-e262-4c8b-aef7-f5562a76bf3b",
   "metadata": {},
   "source": [
    "## RMSE for linear regression maxIter=10, regParam=0.3, elasticNetParam=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ac86fb-3881-4b64-b969-013ecd805625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading results from s3a://sagemaker-us-east-1-260236516028/result/lr/lr-10-0.3-0.8/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 88,233x2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.03432\n",
      "CPU times: user 16.9 ms, sys: 215 µs, total: 17.1 ms\n",
      "Wall time: 6.63 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"*\"\n",
    "print(f\"reading results from {s3_path}\")\n",
    "lr_1 = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {lr_1.count():,}x{len(lr_1.columns)}\")\n",
    "# calculate the rmse\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(lr_1)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479df24-561b-41e1-9086-f9d26b24d416",
   "metadata": {},
   "source": [
    "## RMSE for linear regression maxIter=10, regParam=0.3, elasticNetParam=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67333af-89df-440a-9e07-6b50679c5991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading results from s3a://sagemaker-us-east-1-260236516028/result/lr/lr-10-0.3-0.2/*.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 88,233x2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.99234\n",
      "CPU times: user 14.5 ms, sys: 42 µs, total: 14.6 ms\n",
      "Wall time: 6.07 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"*\"\n",
    "print(f\"reading results from {s3_path}\")\n",
    "lr_2 = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {lr_2.count():,}x{len(lr_2.columns)}\")\n",
    "# calculate the rmse\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(lr_2)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e71986-bfca-4303-be8c-0639a3b010f9",
   "metadata": {},
   "source": [
    "## Calculate r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b1a21a-1ddb-4013-b527-78e4d133deac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def r2_calculate(observed, predicted):\n",
    "    # calculate mean of observed values\n",
    "    mean_observed = np.mean(observed)\n",
    "    # calculate SSR\n",
    "    ssr = np.sum((observed - predicted) ** 2)\n",
    "    # calculate SST\n",
    "    sst = np.sum((observed - mean_observed) ** 2)\n",
    "    # calculate R-squared\n",
    "    r2 = 1 - (ssr / sst)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bede0b7-3920-4505-97d0-8ae467bcad63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for lr_1 is: 0.0396494744260002\n"
     ]
    }
   ],
   "source": [
    "df_lr_1 = lr_1.toPandas()\n",
    "observed = df_lr_1['score']\n",
    "predicted = df_lr_1['prediction']\n",
    "lr_1_r2 = r2_calculate(observed, predicted)\n",
    "print(\"r2 for lr_1 is:\", lr_1_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afde9516-866c-46ec-94a3-66e5a3107db6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for lr_2 is: 0.059530254489551626\n"
     ]
    }
   ],
   "source": [
    "df_lr_2 = lr_2.toPandas()\n",
    "observed = df_lr_2['score']\n",
    "predicted = df_lr_2['prediction']\n",
    "lr_2_r2 = r2_calculate(observed, predicted)\n",
    "print(\"r2 for lr_2 is:\", lr_2_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c51bdc44-27be-4f97-a0b9-724af4d3765d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for lr_1 is: 0.07790613729149365\n"
     ]
    }
   ],
   "source": [
    "df_rf_1 = rf_1.toPandas()\n",
    "observed = df_rf_1['score']\n",
    "predicted = df_rf_1['prediction']\n",
    "rf_1_r2 = r2_calculate(observed, predicted)\n",
    "print(\"r2 for lr_1 is:\", rf_1_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df631adb-38b4-419c-b832-f2c4802c27ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 for lr_1 is: 0.09935082248779104\n"
     ]
    }
   ],
   "source": [
    "df_rf_2 = rf_2.toPandas()\n",
    "observed = df_rf_2['score']\n",
    "predicted = df_rf_2['prediction']\n",
    "rf_2_r2 = r2_calculate(observed, predicted)\n",
    "print(\"r2 for lr_1 is:\", rf_2_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0d0847-409a-40d9-80eb-c254f0d84422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.concat([df_lr_1, df_lr_2, df_rf_1, df_rf_2], axis=1)\n",
    "result.columns = ['score', 'lr-1', 'new_col3', 'lr-2', 'new_col5', 'rf-1', 'new_col7', 'rf-2']\n",
    "columns_to_remove = ['new_col3', 'new_col5', 'new_col7']\n",
    "result = result.drop(columns=columns_to_remove)\n",
    "\n",
    "# calculate the residuals\n",
    "result['re-lr-1'] = result['lr-1'] - result['score']\n",
    "result['re-lr-2'] = result['lr-2'] - result['score']\n",
    "result['re-rf-1'] = result['rf-1'] - result['score']\n",
    "result['re-rf-2'] = result['rf-2'] - result['score']\n",
    "\n",
    "result.to_csv('../../data/csv/score_residual.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d0368-6f57-4e25-9b2a-c5777a8f99c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
